{
  "fileName": "NaviModelClient.scala",
  "filePath": "product-mixer/component-library/src/main/scala/com/twitter/product_mixer/component_library/scorer/common/NaviModelClient.scala",
  "url": "https://github.com/misbahsy/the-algorithm/product-mixer/component-library/src/main/scala/com/twitter/product_mixer/component_library/scorer/common/NaviModelClient.scala",
  "summary": "The `NaviModelClient` class is a client wrapper for calling a Navi Inference Service. It takes in a Finagle HTTP client and a Wily path to the ML Model service as parameters. The purpose of this class is to provide a way to score a model using the Navi Inference Service.\n\nThe `NaviModelClient` class uses the `FinagleChannelBuilder` to create a managed channel for the gRPC connection to the Navi Inference Service. It sets the target to the provided model path and sets the HTTP client to the provided Finagle HTTP client. It also overrides the authority name to \"rustserving\" as required by Navi. Additionally, it enables retry for certain gRPC errors and uses plaintext as mTLS is enabled at the HTTP client level.\n\nThe `score` method takes in a `ModelInferRequest` and returns a `Stitch` of `ModelInferResponse`. It first adapts the `ModelInferRequest` to a `TFServingInferenceRequest` using the `TFServingInferenceServiceImpl.adaptModelInferRequest` method. It then calls the `predict` method of the `inferenceServiceStub` with the adapted request and converts the resulting `ListenableFuture` to a Twitter `Future`. It then maps the response to a `ModelInferResponse` using the `TFServingInferenceServiceImpl.adaptModelInferResponse` method.\n\nOverall, the `NaviModelClient` class provides a way to score a model using the Navi Inference Service by creating a gRPC connection to the service and adapting the request and response to the required formats. It can be used in the larger project to integrate with the Navi Inference Service and score models. An example usage of this class would be:\n\n```\nval httpClient = Http.newClient(\"navi.inference.service:80\")\nval modelPath = \"/s/role/service/model\"\nval client = NaviModelClient(httpClient, modelPath)\nval request = ModelInferRequest(...)\nval response: Stitch[ModelInferResponse] = client.score(request)\n```",
  "questions": "1. What is the purpose of this code?\n- This code is a client wrapper for calling a Navi Inference Service using a Finagle HTTP Client and TensorFlow Serving.\n\n2. What dependencies are required for this code to run?\n- This code requires dependencies from Finagle, TensorFlow Serving, and gRPC.\n\n3. What is the expected input and output of the `score` method?\n- The `score` method expects a `ModelInferRequest` object as input and returns a `Stitch` object containing a `ModelInferResponse`. The `ModelInferRequest` and `ModelInferResponse` objects are defined in the `inference.GrpcService` package."
}